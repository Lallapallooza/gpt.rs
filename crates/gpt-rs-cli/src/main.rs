use anyhow::{anyhow, bail, ensure, Context, Result};
use clap::{Args as ClapArgs, Parser, Subcommand, ValueEnum};
use gpt_rs::backend::conversion::{self, ConversionOptions};
use gpt_rs::backend::spec::Program;
use gpt_rs::backend::text_ir::parse_program;
use gpt_rs::inference::generate::Generator;
use gpt_rs::inference::sampler::Sampler;
use gpt_rs::io::tensor_archive::TensorArchive;
use gpt_rs::ops::trace::{self, FileTraceOptions, FileTraceSink, ProgramDumpFilter};
use gpt_rs::params::ModelNamespaceId;
use gpt_rs::profiling;
use gpt_rs::runtime;
use gpt_rs::tensor::{DeviceTensor, Shape, Tensor};
use gpt_rs::tokenizer::{Tokenizer, TokenizerConfig};
use gpt_rs::PortableBackend;
#[cfg(feature = "conversion-c")]
use gpt_rs_backend_c::CBackend;
use gpt_rs_backend_faer::FaerCpuBackend;
#[cfg(feature = "backend-triton")]
use gpt_rs_backend_triton::TritonBackend;
use rand::SeedableRng;
use rand_chacha::ChaCha8Rng;
use std::collections::HashMap;
use std::env;
use std::fs;
use std::io::{self, Write};
use std::path::{Path, PathBuf};
use std::sync::Arc;

mod benchmark;

use benchmark::compile_freeze::{self, CompileFreezeGate};

fn model_namespace_for_backend<B: PortableBackend + ?Sized>(backend: &B) -> ModelNamespaceId {
    if backend.backend_name() == "c" {
        ModelNamespaceId(0)
    } else {
        runtime::next_namespace()
    }
}

fn install_dump_sink(path: &Path, dump_mode: DumpMode) -> Result<trace::TraceGuard> {
    let options = FileTraceOptions {
        dump_filter: match dump_mode {
            DumpMode::All => ProgramDumpFilter::AllExecutions,
            DumpMode::Compile => ProgramDumpFilter::CompileOnly,
        },
        ..Default::default()
    };

    let sink = FileTraceSink::with_options(path.to_path_buf(), options)
        .with_context(|| format!("failed to prepare PTIR dump directory {}", path.display()))?;
    Ok(trace::install_global_sink(Arc::new(sink)))
}

#[derive(Parser)]
#[command(version, about = "gpt.rs model runner")]
struct Args {
    #[command(subcommand)]
    command: Command,

    #[arg(
        long,
        value_name = "NAME",
        global = true,
        help = "Backend to use (overrides $GPTRS_BACKEND)"
    )]
    backend: Option<String>,

    #[arg(
        long,
        value_name = "DIR",
        global = true,
        help = "Dump PTIR programs and metadata to the specified directory"
    )]
    dump_dir: Option<PathBuf>,

    #[arg(
        long,
        value_enum,
        global = true,
        default_value = "all",
        help = "Controls which PTIR executions are dumped (all, compile)"
    )]
    dump_mode: DumpMode,

    #[arg(long, global = true, help = "Print gpt-rs profiler tables to stderr")]
    profile: bool,

    #[arg(
        long,
        global = true,
        help = "Show full backend profiling rows without compact filtering"
    )]
    profile_full: bool,

    #[arg(
        long,
        value_name = "DIR",
        global = true,
        help = "Write profiling artifacts to the specified directory"
    )]
    profile_out: Option<PathBuf>,

    #[arg(
        long,
        global = true,
        help = "Write Chrome trace timeline artifact (timeline.chrome_trace.json; requires --profile-out)"
    )]
    profile_timeline: bool,

    #[arg(
        long,
        global = true,
        help = "Write flamegraph artifacts (flamegraph.folded and flamegraph.speedscope.json; requires --profile-out)"
    )]
    profile_flamegraph: bool,
}

#[derive(Copy, Clone, Debug, ValueEnum)]
enum DumpMode {
    #[value(name = "all")]
    All,
    #[value(name = "compile")]
    Compile,
}

#[derive(Clone, Debug, Default)]
struct ProfileArtifactConfig {
    out_dir: Option<PathBuf>,
    write_timeline: bool,
    write_flamegraph: bool,
}

impl ProfileArtifactConfig {
    fn trace_enabled(&self) -> bool {
        self.write_timeline || self.write_flamegraph
    }

    fn artifacts_enabled(&self) -> bool {
        self.out_dir.is_some()
    }
}

struct TraceEnableGuard;

impl Drop for TraceEnableGuard {
    fn drop(&mut self) {
        profiling::trace_disable();
    }
}

#[derive(Subcommand)]
enum Command {
    /// Causal text generation for models that implement `CausalLanguageModel`.
    Generate(GenerateArgs),

    /// Dedicated benchmark modes for causal generation.
    Benchmark(BenchmarkArgs),

    /// Run a model forward pass once (tokens or vision input).
    Forward(ForwardArgs),

    /// Convert a PTIR program into a target IR module.
    Convert(ConvertArgs),

    /// List PTIR patterns generated by `#[ptir_pattern]`.
    Patterns(PatternsArgs),
}

#[derive(ClapArgs, Clone)]
struct GenerateArgs {
    #[arg(long, default_value = "checkpoints/gpt2.bin")]
    checkpoint: PathBuf,

    #[arg(long, default_value = "configs/gpt2_tokenizer.json")]
    tokenizer: PathBuf,

    #[arg(long, default_value = "Hello")]
    prompt: String,

    #[arg(long, default_value_t = 128)]
    max_tokens: usize,

    #[arg(long, default_value_t = 0.8)]
    temperature: f32,

    #[arg(long, default_value_t = false)]
    greedy: bool,

    #[arg(long)]
    top_k: Option<usize>,

    #[arg(long, default_value_t = true)]
    kv_cache: bool,

    #[arg(
        long,
        value_name = "TOKENS",
        help = "Fixed KV-cache capacity for decode (disables power-of-two growth buckets)"
    )]
    kv_cache_capacity: Option<usize>,

    #[arg(
        long,
        default_value_t = 0,
        value_name = "N",
        help = "Exclude the first N generated tokens from timing/profiling (warmup)"
    )]
    warmup_tokens: usize,
}

#[derive(Copy, Clone, Debug, ValueEnum)]
enum BenchmarkMode {
    #[value(name = "prefill")]
    Prefill,
    #[value(name = "first-token")]
    FirstToken,
    #[value(name = "decode-steady")]
    DecodeSteady,
}

#[derive(ClapArgs, Clone)]
struct BenchmarkArgs {
    #[arg(long, default_value = "checkpoints/gpt2.bin")]
    checkpoint: PathBuf,

    #[arg(long, default_value = "configs/gpt2_tokenizer.json")]
    tokenizer: PathBuf,

    #[arg(long, default_value = "Hello")]
    prompt: String,

    #[arg(
        long,
        value_enum,
        default_value = "decode-steady",
        help = "Benchmark mode: prefill | first-token | decode-steady"
    )]
    mode: BenchmarkMode,

    #[arg(
        long,
        default_value_t = 129,
        help = "Number of generated tokens for decode-steady mode"
    )]
    max_tokens: usize,

    #[arg(
        long,
        default_value_t = 1,
        value_name = "N",
        help = "Warmup tokens excluded from decode-steady timing"
    )]
    warmup_tokens: usize,

    #[arg(long, default_value_t = 0.8)]
    temperature: f32,

    #[arg(long, default_value_t = false)]
    greedy: bool,

    #[arg(long)]
    top_k: Option<usize>,

    #[arg(long, default_value_t = true)]
    kv_cache: bool,

    #[arg(
        long,
        value_name = "TOKENS",
        help = "Fixed KV-cache capacity for decode (disables power-of-two growth buckets)"
    )]
    kv_cache_capacity: Option<usize>,

    #[arg(
        long,
        default_value_t = 16,
        value_name = "N",
        help = "Measured decode token threshold after which compile activity is considered an anomaly"
    )]
    compile_freeze_after: usize,

    #[arg(
        long,
        default_value_t = false,
        help = "Fail benchmark when compile activity occurs after --compile-freeze-after"
    )]
    strict_compile_freeze: bool,
}

#[derive(ClapArgs, Clone)]
struct VisionInputArgs {
    #[arg(long)]
    input: Option<PathBuf>,

    #[arg(long, default_value = "input")]
    input_key: String,

    /// Generate a deterministic random input when `--input` is not provided.
    #[arg(long, default_value_t = 0)]
    seed: u64,

    #[arg(long, default_value_t = 1)]
    batch: usize,

    #[arg(long, default_value_t = 224)]
    image_size: usize,
}

#[derive(ClapArgs, Clone)]
struct ForwardArgs {
    /// Model checkpoint (self-describing `GPTRSCHK`).
    #[arg(long)]
    checkpoint: PathBuf,

    /// Tokenizer config to use with `--prompt`.
    #[arg(long)]
    tokenizer: Option<PathBuf>,

    /// Text prompt (requires `--tokenizer`).
    #[arg(long)]
    prompt: Option<String>,

    /// Comma-separated token ids, e.g. `--tokens 1,2,3`.
    #[arg(long, value_delimiter = ',')]
    tokens: Vec<usize>,

    #[command(flatten)]
    vision: VisionInputArgs,

    /// Optional output tensor archive path for logits ("logits" tensor).
    #[arg(long)]
    out: Option<PathBuf>,
}

#[derive(ClapArgs, Clone)]
struct ConvertArgs {
    /// Conversion target name (e.g., "c").
    #[arg(long)]
    target: String,

    /// Input PTIR program (.json or .ptir).
    #[arg(long)]
    ptir: PathBuf,

    /// Output directory for converted IR.
    #[arg(long)]
    out: PathBuf,

    /// Optional entrypoint override.
    #[arg(long)]
    entrypoint: Option<String>,
}

#[derive(ClapArgs, Clone)]
struct PatternsArgs {
    /// Optional pattern target filter (exact match).
    #[arg(long)]
    target: Option<String>,
}

fn load_tokenizer(path: impl AsRef<Path>) -> Result<Tokenizer> {
    let path = path.as_ref();
    let data = fs::read_to_string(path)
        .with_context(|| format!("failed to read tokenizer from {}", path.display()))?;
    let cfg = TokenizerConfig::from_json_str(&data).with_context(|| {
        format!(
            "invalid tokenizer config in {} (expected flat gpt-rs schema or Hugging Face tokenizer.json)",
            path.display()
        )
    })?;
    Ok(Tokenizer::from_config(cfg))
}

fn load_vision_input_tensor(args: &VisionInputArgs) -> Result<Tensor> {
    if let Some(path) = args.input.as_ref() {
        let tensors = TensorArchive::load(path)?;
        let input = tensors.get(&args.input_key).ok_or_else(|| {
            anyhow!(
                "input archive {} missing tensor '{}'",
                path.display(),
                args.input_key
            )
        })?;
        return Ok(input.clone());
    }

    ensure!(
        args.batch > 0,
        "--batch must be > 0 when generating a random input",
    );
    ensure!(
        args.image_size > 0,
        "--image-size must be > 0 when generating a random input",
    );

    let mut rng = ChaCha8Rng::seed_from_u64(args.seed);
    Ok(Tensor::randn(
        Shape::new([args.batch, 3, args.image_size, args.image_size]),
        1.0,
        &mut rng,
    ))
}

fn main() -> Result<()> {
    let args = Args::parse();
    let artifact_config = ProfileArtifactConfig {
        out_dir: args.profile_out.clone(),
        write_timeline: args.profile_timeline,
        write_flamegraph: args.profile_flamegraph,
    };
    if artifact_config.trace_enabled() && artifact_config.out_dir.is_none() {
        bail!("--profile-timeline and --profile-flamegraph require --profile-out <DIR>");
    }
    let profile_enabled = args.profile || args.profile_full || artifact_config.artifacts_enabled();
    let _trace_guard = args
        .dump_dir
        .as_ref()
        .map(|dir| install_dump_sink(dir, args.dump_mode))
        .transpose()?;
    let _profile_trace_guard = if profile_enabled && artifact_config.trace_enabled() {
        profiling::trace_reset();
        profiling::trace_enable();
        Some(TraceEnableGuard)
    } else {
        None
    };

    #[cfg(feature = "conversion-c")]
    gpt_rs_backend_c::register_conversion_targets();
    #[cfg(feature = "backend-triton")]
    gpt_rs_backend_triton::register_conversion_targets();

    if profile_enabled {
        profiling::reset();
    }

    if matches!(args.command, Command::Convert(_)) {
        return run_conversion(match args.command {
            Command::Convert(convert) => convert,
            _ => unreachable!(),
        });
    }

    if matches!(args.command, Command::Patterns(_)) {
        return run_patterns(match args.command {
            Command::Patterns(patterns) => patterns,
            _ => unreachable!(),
        });
    }

    let backend_env = args
        .backend
        .unwrap_or_else(|| env::var("GPTRS_BACKEND").unwrap_or_else(|_| "faer".to_string()));
    let command = args.command;
    let supported = supported_backends();
    let profile_consumed = match backend_env.trim() {
        "faer" => {
            let backend = Arc::new(FaerCpuBackend::create());
            run_with_backend(
                backend,
                command,
                profile_enabled,
                args.profile_full,
                &artifact_config,
            )?
        }
        "c" => {
            #[cfg(feature = "conversion-c")]
            {
                let backend = Arc::new(CBackend::new());
                run_with_backend(
                    backend,
                    command,
                    profile_enabled,
                    args.profile_full,
                    &artifact_config,
                )?
            }
            #[cfg(not(feature = "conversion-c"))]
            {
                bail!(
                    "GPTRS_BACKEND='c' requires building gpt-rs-cli with --features conversion-c"
                );
            }
        }
        "triton" => {
            #[cfg(feature = "backend-triton")]
            {
                // Per-op CUDA event timing is only enabled for `--profile-full`.
                gpt_rs_backend_triton::set_gpu_event_timing(profile_enabled && args.profile_full);
                let backend = Arc::new(TritonBackend::new());
                run_with_backend(
                    backend,
                    command,
                    profile_enabled,
                    args.profile_full,
                    &artifact_config,
                )?
            }
            #[cfg(not(feature = "backend-triton"))]
            {
                bail!(
                    "GPTRS_BACKEND='triton' requires building gpt-rs-cli with --features backend-triton"
                );
            }
        }
        other => {
            bail!(
                "unknown backend '{}'; available: {}",
                other,
                supported.join(", ")
            );
        }
    };

    if profile_enabled && !profile_consumed {
        let options = profiling::ProfileFormatOptions {
            profile_full: args.profile_full,
            ..profiling::ProfileFormatOptions::default()
        };
        if let Some(snapshot) = profiling::take_profile_snapshot_with_options(false, &options) {
            if let Some(formatted) = snapshot.formatted_str() {
                eprintln!("{formatted}");
            }
            write_profile_artifacts(&snapshot, &artifact_config)?;
        } else {
            eprintln!(
                "profiling enabled but no report available; rebuild gpt-rs with profiler support"
            );
        }
    }

    Ok(())
}

fn supported_backends() -> Vec<&'static str> {
    [
        Some("faer"),
        cfg!(feature = "backend-triton").then_some("triton"),
        cfg!(feature = "conversion-c").then_some("c"),
    ]
    .into_iter()
    .flatten()
    .collect()
}

fn run_with_backend<B: PortableBackend + 'static>(
    backend: Arc<B>,
    command: Command,
    profile: bool,
    profile_full: bool,
    artifact_config: &ProfileArtifactConfig,
) -> Result<bool> {
    // Keep profiler instrumentation compiled in, but suspend runtime collection unless
    // the user explicitly asked for a report via `--profile`.
    let _suspend_profiler = if profile {
        None
    } else {
        Some(profiling::suspend())
    };

    match command {
        Command::Generate(args) => {
            run_generate(&backend, &args, profile)?;
            Ok(false)
        }
        Command::Benchmark(args) => {
            run_benchmark(&backend, &args, profile, profile_full, artifact_config)
        }
        Command::Forward(args) => {
            run_forward(&backend, &args, profile)?;
            Ok(false)
        }
        Command::Convert(_) => unreachable!(),
        Command::Patterns(_) => unreachable!(),
    }
}

fn run_patterns(args: PatternsArgs) -> Result<()> {
    let mut defs: Vec<_> = gpt_rs::backend::pattern::all_pattern_defs()
        .iter()
        .collect();
    defs.sort_by_key(|def| def.target);

    let mut printed = 0usize;
    for def in defs {
        if args
            .target
            .as_deref()
            .is_some_and(|target| target != def.target)
        {
            continue;
        }
        println!("{} -> {}::{}", def.target, def.module_path, def.view_name);
        for field in def.fields {
            if field.optional {
                println!("  {}: Option<{}>", field.name, field.view);
            } else {
                println!("  {}: {}", field.name, field.view);
            }
        }
        printed += 1;
    }

    if printed == 0 {
        if let Some(target) = args.target {
            println!("no patterns found for target {target}");
        } else {
            println!(
                "no patterns registered (did you compile gpt-rs with any #[ptir_pattern] sites?)"
            );
        }
    }

    Ok(())
}

fn run_generate<B: PortableBackend + 'static>(
    backend: &Arc<B>,
    args: &GenerateArgs,
    profile: bool,
) -> Result<()> {
    let namespace = model_namespace_for_backend(backend.as_ref());
    let model =
        runtime::load_model_with_namespace(Arc::clone(backend), &args.checkpoint, namespace)
            .with_context(|| format!("failed to load checkpoint {}", args.checkpoint.display()))?;
    let lm = model.as_causal_lm().ok_or_else(|| {
        anyhow!(
            "model kind '{}' does not support causal generation",
            model.kind()
        )
    })?;
    let tokenizer = load_tokenizer(&args.tokenizer)?;

    let prompt_tokens = tokenizer.encode(&args.prompt);
    ensure!(
        !prompt_tokens.is_empty(),
        "prompt produced an empty token sequence"
    );
    let kv_cache_capacity = resolve_kv_cache_capacity(
        args.kv_cache,
        args.kv_cache_capacity,
        prompt_tokens.len(),
        args.max_tokens,
        lm.context_length(),
    );

    let mut sampler = Sampler::new(args.temperature);
    if args.greedy {
        sampler.temperature = 0.0;
    }
    if let Some(k) = args.top_k {
        sampler.top_k = Some(k.max(1));
    }

    let mut stdout = io::stdout();
    let mut pending_output = String::new();
    let mut pending_tokens = 0usize;
    let warmup = args.warmup_tokens.min(args.max_tokens);

    if warmup == 0 {
        if profile {
            profiling::reset();
        }
        let t0 = std::time::Instant::now();
        let mut model_elapsed = std::time::Duration::ZERO;
        let mut first_step_elapsed = std::time::Duration::ZERO;

        let mut generator = Generator::new_with_kv_cache_capacity(
            lm,
            &sampler,
            &prompt_tokens,
            args.kv_cache,
            kv_cache_capacity,
        )?;

        for step in 0..args.max_tokens {
            let step_t0 = std::time::Instant::now();
            let token = if step + 1 == args.max_tokens {
                generator.step_final()?
            } else {
                generator.step()?
            };
            let step_elapsed = step_t0.elapsed();
            model_elapsed += step_elapsed;
            if step == 0 {
                first_step_elapsed = step_elapsed;
            }
            let piece = tokenizer.decode(&[token]);
            stream_piece(
                &mut stdout,
                &mut pending_output,
                &mut pending_tokens,
                piece.as_str(),
            )?;
        }

        let out = generator.into_tokens();
        let wall_elapsed = t0.elapsed();

        flush_stream_buffer(&mut stdout, &mut pending_output, &mut pending_tokens)?;
        stdout.write_all(b"\n")?;
        stdout.flush()?;

        let emitted = out
            .len()
            .saturating_sub(prompt_tokens.len())
            .min(args.max_tokens);
        eprintln!(
            "Generated {} tokens in {:.2?} ({:.2} tokens/sec)",
            emitted,
            model_elapsed,
            (emitted as f64) / model_elapsed.as_secs_f64().max(1e-9),
        );
        if emitted > 1 {
            let steady_tokens = emitted - 1;
            let steady_elapsed = model_elapsed.saturating_sub(first_step_elapsed);
            eprintln!(
                "Decode steady-state (excluding first token): {:.2} tokens/sec",
                (steady_tokens as f64) / steady_elapsed.as_secs_f64().max(1e-9)
            );
        }
        eprintln!("Wall time including decode/output: {:.2?}", wall_elapsed);

        return Ok(());
    }

    let mut generator = Generator::new_with_kv_cache_capacity(
        lm,
        &sampler,
        &prompt_tokens,
        args.kv_cache,
        kv_cache_capacity,
    )?;

    let measured_steps = args.max_tokens.saturating_sub(warmup);
    let warmup_is_final = measured_steps == 0 && warmup > 0;

    for step in 0..warmup {
        let token = if warmup_is_final && step + 1 == warmup {
            generator.step_final()?
        } else {
            generator.step()?
        };
        let piece = tokenizer.decode(&[token]);
        stream_piece(
            &mut stdout,
            &mut pending_output,
            &mut pending_tokens,
            piece.as_str(),
        )?;
    }

    if profile {
        profiling::reset();
    }
    let t0 = std::time::Instant::now();
    let mut model_elapsed = std::time::Duration::ZERO;

    for step in 0..measured_steps {
        let step_t0 = std::time::Instant::now();
        let token = if step + 1 == measured_steps {
            generator.step_final()?
        } else {
            generator.step()?
        };
        model_elapsed += step_t0.elapsed();
        let piece = tokenizer.decode(&[token]);
        stream_piece(
            &mut stdout,
            &mut pending_output,
            &mut pending_tokens,
            piece.as_str(),
        )?;
    }

    let out = generator.into_tokens();
    let wall_elapsed = t0.elapsed();

    flush_stream_buffer(&mut stdout, &mut pending_output, &mut pending_tokens)?;
    stdout.write_all(b"\n")?;
    stdout.flush()?;

    let emitted = out
        .len()
        .saturating_sub(prompt_tokens.len())
        .min(args.max_tokens);
    let measured = emitted.saturating_sub(warmup);
    eprintln!(
        "Generated {} tokens ({} warmup) in {:.2?} ({:.2} tokens/sec)",
        measured,
        warmup,
        model_elapsed,
        (measured as f64) / model_elapsed.as_secs_f64().max(1e-9),
    );
    eprintln!(
        "Wall time including decode/output for measured tokens: {:.2?}",
        wall_elapsed
    );

    Ok(())
}

const STREAM_FLUSH_TOKENS: usize = 16;
const STREAM_FLUSH_BYTES: usize = 4096;

fn stream_piece(
    stdout: &mut io::Stdout,
    pending_output: &mut String,
    pending_tokens: &mut usize,
    piece: &str,
) -> Result<()> {
    pending_output.push_str(piece);
    *pending_tokens = pending_tokens.saturating_add(1);
    if *pending_tokens >= STREAM_FLUSH_TOKENS || pending_output.len() >= STREAM_FLUSH_BYTES {
        flush_stream_buffer(stdout, pending_output, pending_tokens)?;
    }
    Ok(())
}

fn flush_stream_buffer(
    stdout: &mut io::Stdout,
    pending_output: &mut String,
    pending_tokens: &mut usize,
) -> Result<()> {
    if pending_output.is_empty() {
        return Ok(());
    }
    stdout.write_all(pending_output.as_bytes())?;
    pending_output.clear();
    *pending_tokens = 0;
    stdout.flush()?;
    Ok(())
}

fn resolve_kv_cache_capacity(
    kv_cache_enabled: bool,
    requested_capacity: Option<usize>,
    prompt_len: usize,
    max_tokens: usize,
    context_length: usize,
) -> Option<usize> {
    if !kv_cache_enabled {
        return None;
    }
    if let Some(capacity) = requested_capacity {
        return Some(capacity);
    }
    let required = prompt_len.saturating_add(max_tokens).max(1);
    let auto_capacity = required.min(context_length);
    if auto_capacity == 0 {
        None
    } else {
        Some(auto_capacity)
    }
}

fn run_benchmark<B: PortableBackend + 'static>(
    backend: &Arc<B>,
    args: &BenchmarkArgs,
    profile: bool,
    profile_full: bool,
    artifact_config: &ProfileArtifactConfig,
) -> Result<bool> {
    let namespace = model_namespace_for_backend(backend.as_ref());
    let model =
        runtime::load_model_with_namespace(Arc::clone(backend), &args.checkpoint, namespace)
            .with_context(|| format!("failed to load checkpoint {}", args.checkpoint.display()))?;
    let lm = model.as_causal_lm().ok_or_else(|| {
        anyhow!(
            "model kind '{}' does not support causal generation",
            model.kind()
        )
    })?;
    let tokenizer = load_tokenizer(&args.tokenizer)?;
    let prompt_tokens = tokenizer.encode(&args.prompt);
    ensure!(
        !prompt_tokens.is_empty(),
        "prompt produced an empty token sequence"
    );
    let kv_cache_capacity = resolve_kv_cache_capacity(
        args.kv_cache,
        args.kv_cache_capacity,
        prompt_tokens.len(),
        args.max_tokens,
        lm.context_length(),
    );

    let mut sampler = Sampler::new(args.temperature);
    if args.greedy {
        sampler.temperature = 0.0;
    }
    if let Some(k) = args.top_k {
        sampler.top_k = Some(k.max(1));
    }

    match args.mode {
        BenchmarkMode::Prefill => {
            if profile {
                profiling::reset();
            }
            let elapsed = {
                let _scope = profile.then(|| profiling::backend_scope("benchmark.prefill_total"));
                let t0 = std::time::Instant::now();
                let _generator = Generator::new_with_kv_cache_capacity(
                    lm,
                    &sampler,
                    &prompt_tokens,
                    args.kv_cache,
                    kv_cache_capacity,
                )?;
                t0.elapsed()
            };
            eprintln!(
                "Benchmark prefill latency: prompt_tokens={} elapsed={:.2?}",
                prompt_tokens.len(),
                elapsed
            );

            if profile {
                emit_profile_with_attribution(
                    elapsed,
                    1.0,
                    Some("run"),
                    "prefill",
                    profile_full,
                    artifact_config,
                    None,
                )?;
                return Ok(true);
            }
            Ok(false)
        }
        BenchmarkMode::FirstToken => {
            if profile {
                profiling::reset();
            }
            let (token, elapsed) = {
                let _scope =
                    profile.then(|| profiling::backend_scope("benchmark.first_token_total"));
                let t0 = std::time::Instant::now();
                let mut generator = Generator::new_with_kv_cache_capacity(
                    lm,
                    &sampler,
                    &prompt_tokens,
                    args.kv_cache,
                    kv_cache_capacity,
                )?;
                let token = generator.step_final()?;
                (token, t0.elapsed())
            };
            eprintln!(
                "Benchmark first-token latency: prompt_tokens={} token={} elapsed={:.2?} ({:.2} tokens/sec)",
                prompt_tokens.len(),
                token,
                elapsed,
                1.0f64 / elapsed.as_secs_f64().max(1e-9)
            );

            if profile {
                emit_profile_with_attribution(
                    elapsed,
                    1.0,
                    Some("run"),
                    "first-token",
                    profile_full,
                    artifact_config,
                    None,
                )?;
                return Ok(true);
            }
            Ok(false)
        }
        BenchmarkMode::DecodeSteady => {
            ensure!(
                args.max_tokens > 0,
                "--max-tokens must be > 0 for decode-steady benchmark"
            );
            let warmup = args.warmup_tokens.min(args.max_tokens);
            let measured_steps = args.max_tokens.saturating_sub(warmup);
            ensure!(
                measured_steps > 0,
                "decode-steady requires at least one measured token; got max_tokens={} warmup_tokens={}",
                args.max_tokens,
                args.warmup_tokens
            );

            let mut generator = Generator::new_with_kv_cache_capacity(
                lm,
                &sampler,
                &prompt_tokens,
                args.kv_cache,
                kv_cache_capacity,
            )?;

            for _ in 0..warmup {
                generator.step()?;
            }

            if profile {
                profiling::reset();
            }
            let freeze_capture_token = args.compile_freeze_after.min(measured_steps);
            let mut compile_baseline_ms = if profile && freeze_capture_token == 0 {
                profiling::take_profile_snapshot_with_options(
                    false,
                    &profiling::ProfileFormatOptions::default(),
                )
                .and_then(|snapshot| {
                    snapshot
                        .report_json_str()
                        .and_then(compile_freeze::compile_ms_from_report_json)
                })
            } else {
                None
            };
            let elapsed = {
                let _scope =
                    profile.then(|| profiling::backend_scope("benchmark.decode_steady_total"));
                let t0 = std::time::Instant::now();
                for step in 0..measured_steps {
                    if step + 1 == measured_steps {
                        generator.step_final()?;
                    } else {
                        generator.step()?;
                    }
                    if profile && step + 1 == freeze_capture_token {
                        compile_baseline_ms = profiling::take_profile_snapshot_with_options(
                            false,
                            &profiling::ProfileFormatOptions::default(),
                        )
                        .and_then(|snapshot| {
                            snapshot
                                .report_json_str()
                                .and_then(compile_freeze::compile_ms_from_report_json)
                        });
                    }
                }
                t0.elapsed()
            };
            let tps = (measured_steps as f64) / elapsed.as_secs_f64().max(1e-9);
            eprintln!(
                "Benchmark decode-steady: measured_tokens={} warmup_tokens={} elapsed={:.2?} ({:.2} tokens/sec)",
                measured_steps,
                warmup,
                elapsed,
                tps
            );

            if profile {
                emit_profile_with_attribution(
                    elapsed,
                    measured_steps as f64,
                    Some("token"),
                    "decode-steady",
                    profile_full,
                    artifact_config,
                    Some(CompileFreezeGate {
                        measured_token_threshold: args.compile_freeze_after,
                        strict: args.strict_compile_freeze,
                        baseline_compile_ms: compile_baseline_ms,
                    }),
                )?;
                return Ok(true);
            }
            Ok(false)
        }
    }
}

fn emit_profile_with_attribution(
    elapsed: std::time::Duration,
    measured_units: f64,
    unit_label: Option<&str>,
    label: &str,
    profile_full: bool,
    artifact_config: &ProfileArtifactConfig,
    compile_freeze_gate: Option<CompileFreezeGate>,
) -> Result<()> {
    let format_options = profiling::ProfileFormatOptions {
        measured_units: Some(measured_units).filter(|units| *units > 0.0),
        unit_label: unit_label.map(str::to_string),
        profile_full,
        ..profiling::ProfileFormatOptions::default()
    };
    if let Some(snapshot) = profiling::take_profile_snapshot_with_options(false, &format_options) {
        let report_json = snapshot.report_json_str().unwrap_or("");
        if let Some(gate) = compile_freeze_gate {
            match compile_freeze::evaluate_compile_freeze(report_json, gate) {
                Some(observation) if observation.has_anomaly() => {
                    eprintln!(
                        "Compile freeze check: compile activity detected after measured token {} ({:.3} ms total)",
                        gate.measured_token_threshold,
                        observation.compile_ms_after_threshold
                    );
                    if gate.strict {
                        bail!(
                            "strict compile freeze violation: compile activity ({:.3} ms) occurred after measured token {}",
                            observation.compile_ms_after_threshold,
                            gate.measured_token_threshold
                        );
                    }
                }
                Some(_) => {
                    eprintln!(
                        "Compile freeze check: no compile activity detected after measured token {}",
                        gate.measured_token_threshold
                    );
                }
                None => {
                    eprintln!(
                        "Compile freeze check: unavailable (missing profiler baseline or compile tables)"
                    );
                }
            }
        }
        if let Some(component_ms) = measured_component_ms_from_report_json(report_json) {
            let wall_ms = elapsed.as_secs_f64() * 1_000.0;
            let coverage = if wall_ms > 0.0 {
                (component_ms / wall_ms) * 100.0
            } else {
                0.0
            };
            let clamped = coverage.clamp(0.0, 100.0);
            let per_unit_ms = if measured_units > 0.0 {
                wall_ms / measured_units
            } else {
                wall_ms
            };
            eprintln!(
                "Attribution ({label}): {:.1}% measured ({:.3} ms profiled / {:.3} ms wall, {:.3} ms/unit)",
                clamped, component_ms, wall_ms, per_unit_ms
            );
            if clamped < 90.0 {
                eprintln!(
                    "Attribution warning: measured coverage is below 90% (current {:.1}%).",
                    clamped
                );
            }
        } else {
            eprintln!("Attribution ({label}): unavailable (failed to parse profiler report).");
        }
        if let Some(formatted) = snapshot.formatted_str() {
            eprintln!("{formatted}");
        }
        write_profile_artifacts(&snapshot, artifact_config)?;
    } else {
        eprintln!(
            "profiling enabled but no report available; rebuild gpt-rs with profiler support"
        );
    }
    Ok(())
}

fn write_profile_artifacts(
    snapshot: &profiling::ProfileSnapshot,
    artifact_config: &ProfileArtifactConfig,
) -> Result<()> {
    let Some(out_dir) = artifact_config.out_dir.as_ref() else {
        return Ok(());
    };
    fs::create_dir_all(out_dir)
        .with_context(|| format!("failed to create profile output dir {}", out_dir.display()))?;

    let jsonl_path = out_dir.join("profile.jsonl");
    let profile_jsonl = snapshot.profile_jsonl_str().unwrap_or("");
    fs::write(&jsonl_path, profile_jsonl.as_bytes())
        .with_context(|| format!("failed to write {}", jsonl_path.display()))?;

    if artifact_config.write_timeline || artifact_config.write_flamegraph {
        let Some(trace_json) = profiling::take_trace_json() else {
            return Err(anyhow!(
                "trace export requested but no trace events were captured; ensure profiler support is enabled"
            ));
        };
        if artifact_config.write_timeline {
            let timeline_path = out_dir.join("timeline.chrome_trace.json");
            fs::write(&timeline_path, trace_json.as_bytes())
                .with_context(|| format!("failed to write {}", timeline_path.display()))?;
        }
        if artifact_config.write_flamegraph {
            let folded =
                profiling::chrome_trace_to_folded(trace_json.as_str()).ok_or_else(|| {
                    anyhow!("failed to convert Chrome trace into folded flamegraph format")
                })?;
            let folded_path = out_dir.join("flamegraph.folded");
            fs::write(&folded_path, folded.as_bytes())
                .with_context(|| format!("failed to write {}", folded_path.display()))?;

            let speedscope = profiling::chrome_trace_to_speedscope(trace_json.as_str())
                .ok_or_else(|| anyhow!("failed to convert Chrome trace into speedscope format"))?;
            let speedscope_path = out_dir.join("flamegraph.speedscope.json");
            fs::write(&speedscope_path, speedscope.as_bytes())
                .with_context(|| format!("failed to write {}", speedscope_path.display()))?;
        }
    }
    Ok(())
}

fn measured_component_ms_from_report_json(report_json: &str) -> Option<f64> {
    let parsed: serde_json::Value = serde_json::from_str(report_json).ok()?;
    let sections = parsed.get("sections")?.as_array()?;
    let mut total = 0.0f64;

    for section in sections {
        let tables = section.get("tables")?;
        for key in [
            "layers",
            "functionals",
            "backend",
            "compilation",
            "compile_passes",
        ] {
            if let Some(rows) = tables.get(key).and_then(|v| v.as_array()) {
                for row in rows {
                    total += row.get("excl_ms").and_then(|v| v.as_f64()).unwrap_or(0.0);
                }
            }
        }
    }
    Some(total)
}

fn run_conversion(args: ConvertArgs) -> Result<()> {
    let program = load_ptir_program(&args.ptir)?;
    let target = conversion::get_conversion_target(&args.target).ok_or_else(|| {
        anyhow!(
            "unknown conversion target '{}'; available: {}",
            args.target,
            conversion::list_conversion_targets().join(", ")
        )
    })?;

    let options = ConversionOptions {
        entrypoint_override: args.entrypoint.clone(),
    };

    target
        .check(&program, &options)
        .map_err(|err| anyhow!(err.to_string()))?;
    let converted = target
        .convert(&program, &options)
        .map_err(|err| anyhow!(err.to_string()))?;

    fs::create_dir_all(&args.out)
        .with_context(|| format!("failed to create output dir {}", args.out.display()))?;
    let filename = format!("program.{}", target.file_extension());
    let out_path = args.out.join(filename);
    fs::write(&out_path, converted.module)
        .with_context(|| format!("failed to write {}", out_path.display()))?;

    let meta_path = args.out.join("meta.json");
    let meta = serde_json::json!({
        "target": target.name(),
        "entrypoints": converted.entrypoints.iter().map(|e| {
            serde_json::json!({ "ptir": e.ptir, "symbol": e.symbol })
        }).collect::<Vec<_>>(),
    });
    fs::write(&meta_path, serde_json::to_string_pretty(&meta)?)
        .with_context(|| format!("failed to write {}", meta_path.display()))?;

    eprintln!(
        "Converted PTIR '{}' to {}",
        args.ptir.display(),
        out_path.display()
    );
    Ok(())
}

fn load_ptir_program(path: &Path) -> Result<Program> {
    let ext = path.extension().and_then(|s| s.to_str()).unwrap_or("");
    match ext {
        "json" => Program::load_json(path).map_err(|err| anyhow!(err.to_string())),
        "ptir" | "txt" => {
            let src = fs::read_to_string(path)
                .with_context(|| format!("failed to read {}", path.display()))?;
            if src.trim_start().starts_with("program @") {
                bail!("text PTIR program parsing is not supported yet; use the JSON dump instead");
            }
            parse_program(&src).map_err(|err| anyhow!(err.to_string()))
        }
        _ => {
            let json = Program::load_json(path);
            if let Ok(program) = json {
                return Ok(program);
            }
            let src = fs::read_to_string(path)
                .with_context(|| format!("failed to read {}", path.display()))?;
            if src.trim_start().starts_with("program @") {
                bail!("text PTIR program parsing is not supported yet; use the JSON dump instead");
            }
            parse_program(&src).map_err(|err| anyhow!(err.to_string()))
        }
    }
}

fn run_forward<B: PortableBackend + 'static>(
    backend: &Arc<B>,
    args: &ForwardArgs,
    profile: bool,
) -> Result<()> {
    let namespace = model_namespace_for_backend(backend.as_ref());
    let mut model =
        runtime::load_model_with_namespace(Arc::clone(backend), &args.checkpoint, namespace)
            .with_context(|| format!("failed to load checkpoint {}", args.checkpoint.display()))?;

    let is_text = args.prompt.is_some() || !args.tokens.is_empty();
    if args.prompt.is_some() && !args.tokens.is_empty() {
        bail!("use either --prompt or --tokens, not both");
    }
    if args.tokenizer.is_some() && args.prompt.is_none() {
        bail!("--tokenizer is only valid together with --prompt");
    }
    if is_text && args.vision.input.is_some() {
        bail!("cannot combine text input (--prompt/--tokens) with vision input (--input)");
    }

    let input = if let Some(prompt) = args.prompt.as_ref() {
        let tokenizer_path = args
            .tokenizer
            .as_ref()
            .ok_or_else(|| anyhow!("--prompt requires --tokenizer"))?;
        let tokenizer = load_tokenizer(tokenizer_path)?;
        let tokens = tokenizer.encode(prompt);
        ensure!(
            !tokens.is_empty(),
            "prompt produced an empty token sequence"
        );
        runtime::ModelInput::Tokens(tokens)
    } else if !args.tokens.is_empty() {
        runtime::ModelInput::Tokens(args.tokens.clone())
    } else {
        let input = load_vision_input_tensor(&args.vision)?;
        let input_device = DeviceTensor::from_host(Arc::clone(backend), input)
            .with_context(|| "failed to move input to device")?;
        runtime::ModelInput::Vision(input_device)
    };

    if profile {
        profiling::reset();
    }
    let t0 = std::time::Instant::now();
    let runtime::ModelOutput::Tensor(host) = model.forward(input)?;
    let elapsed = t0.elapsed();

    let model_kind = model.kind();
    let shape = host.shape().dims();
    println!("model={} output_shape={:?}", model_kind, shape);

    if shape.len() == 2 && shape[0] > 0 && shape[1] > 0 {
        let rows = shape[0];
        let cols = shape[1];
        let row = if is_text { rows - 1 } else { 0 };
        let start = row * cols;
        let logits = &host.data()[start..start + cols];
        let mut best = 0usize;
        let mut best_val = f32::NEG_INFINITY;
        for (i, &v) in logits.iter().enumerate() {
            if v > best_val {
                best = i;
                best_val = v;
            }
        }
        println!("top1={} logit={:.6}", best, best_val);
    }

    eprintln!("wall_s={:.3}", elapsed.as_secs_f64());

    if let Some(out) = args.out.as_ref() {
        let mut map = HashMap::new();
        map.insert("logits".to_string(), host);
        TensorArchive::save(out, &map)
            .with_context(|| format!("failed to write output archive {}", out.display()))?;
    }

    Ok(())
}
