# gpt_rs.kernel: transpose_f32_rank5
# gpt_rs.symbol: gpt_rs_triton_transpose_f32_rank5
# gpt_rs.signature: in_ptr=*fp32,out_ptr=*fp32,n=i32,od0=i32,od1=i32,od2=i32,od3=i32,od4=i32,is0=i32,is1=i32,is2=i32,is3=i32,is4=i32
# gpt_rs.param_abi: *fp32,*fp32,u32,i32,i32,i32,i32,i32,i32,i32,i32,i32,i32,*opaque
# gpt_rs.constexpr: BLOCK_SIZE=256
# gpt_rs.num_warps: 8

import triton
import triton.language as tl


@triton.jit
def gpt_rs_triton_transpose_f32_rank5(
    in_ptr,
    out_ptr,
    n,
    od0,
    od1,
    od2,
    od3,
    od4,
    is0,
    is1,
    is2,
    is3,
    is4,
    BLOCK_SIZE: tl.constexpr,
):
    pid = tl.program_id(0)
    idx = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)
    mask = idx < n

    plane34 = od3 * od4
    plane234 = od2 * plane34
    plane1234 = od1 * plane234

    o0 = idx // plane1234
    rem0 = idx - o0 * plane1234
    o1 = rem0 // plane234
    rem1 = rem0 - o1 * plane234
    o2 = rem1 // plane34
    rem2 = rem1 - o2 * plane34
    o3 = rem2 // od4
    o4 = rem2 - o3 * od4

    in_off = o0 * is0 + o1 * is1 + o2 * is2 + o3 * is3 + o4 * is4
    values = tl.load(in_ptr + in_off, mask=mask, other=0.0)
    tl.store(out_ptr + idx, values, mask=mask)
