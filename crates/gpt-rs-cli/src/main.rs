use anyhow::{anyhow, bail, ensure, Context, Result};
use clap::{Args as ClapArgs, Parser, Subcommand, ValueEnum};
use gpt_rs::backend::conversion::{self, ConversionOptions};
use gpt_rs::backend::spec::Program;
use gpt_rs::backend::text_ir::parse_program;
use gpt_rs::inference::generate::Generator;
use gpt_rs::inference::sampler::Sampler;
use gpt_rs::io::tensor_archive::TensorArchive;
use gpt_rs::ops::trace::{self, FileTraceOptions, FileTraceSink, ProgramDumpFilter};
use gpt_rs::params::ModelNamespaceId;
use gpt_rs::profiling;
use gpt_rs::runtime;
use gpt_rs::tensor::{DeviceTensor, Shape, Tensor};
use gpt_rs::tokenizer::{Tokenizer, TokenizerConfig};
use gpt_rs::PortableBackend;
#[cfg(feature = "conversion-c")]
use gpt_rs_backend_c::CBackend;
use gpt_rs_backend_faer::FaerCpuBackend;
use rand::SeedableRng;
use rand_chacha::ChaCha8Rng;
use std::collections::HashMap;
use std::env;
use std::fs;
use std::io::{self, Write};
use std::path::{Path, PathBuf};
use std::sync::Arc;

fn model_namespace_for_backend<B: PortableBackend + ?Sized>(backend: &B) -> ModelNamespaceId {
    if backend.backend_name() == "c" {
        ModelNamespaceId(0)
    } else {
        runtime::next_namespace()
    }
}

fn install_dump_sink(path: &Path, dump_mode: DumpMode) -> Result<trace::TraceGuard> {
    let options = FileTraceOptions {
        dump_filter: match dump_mode {
            DumpMode::All => ProgramDumpFilter::AllExecutions,
            DumpMode::Compile => ProgramDumpFilter::CompileOnly,
        },
        ..Default::default()
    };

    let sink = FileTraceSink::with_options(path.to_path_buf(), options)
        .with_context(|| format!("failed to prepare PTIR dump directory {}", path.display()))?;
    Ok(trace::install_global_sink(Arc::new(sink)))
}

#[derive(Parser)]
#[command(version, about = "gpt.rs model runner")]
struct Args {
    #[command(subcommand)]
    command: Command,

    #[arg(
        long,
        value_name = "NAME",
        global = true,
        help = "Backend to use (overrides $GPTRS_BACKEND)"
    )]
    backend: Option<String>,

    #[arg(
        long,
        value_name = "DIR",
        global = true,
        help = "Dump PTIR programs and metadata to the specified directory"
    )]
    dump_dir: Option<PathBuf>,

    #[arg(
        long,
        value_enum,
        global = true,
        default_value = "all",
        help = "Controls which PTIR executions are dumped (all, compile)"
    )]
    dump_mode: DumpMode,

    #[arg(long, global = true, help = "Print gpt-rs profiler tables to stderr")]
    profile: bool,
}

#[derive(Copy, Clone, Debug, ValueEnum)]
enum DumpMode {
    #[value(name = "all")]
    All,
    #[value(name = "compile")]
    Compile,
}

#[derive(Subcommand)]
enum Command {
    /// Causal text generation for models that implement `CausalLanguageModel`.
    Generate(GenerateArgs),

    /// Run a model forward pass once (tokens or vision input).
    Forward(ForwardArgs),

    /// Convert a PTIR program into a target IR module.
    Convert(ConvertArgs),

    /// List PTIR patterns generated by `#[ptir_pattern]`.
    Patterns(PatternsArgs),
}

#[derive(ClapArgs, Clone)]
struct GenerateArgs {
    #[arg(long, default_value = "checkpoints/gpt2.bin")]
    checkpoint: PathBuf,

    #[arg(long, default_value = "configs/gpt2_tokenizer.json")]
    tokenizer: PathBuf,

    #[arg(long, default_value = "Hello")]
    prompt: String,

    #[arg(long, default_value_t = 128)]
    max_tokens: usize,

    #[arg(long, default_value_t = 0.8)]
    temperature: f32,

    #[arg(long, default_value_t = false)]
    greedy: bool,

    #[arg(long)]
    top_k: Option<usize>,

    #[arg(long, default_value_t = true)]
    kv_cache: bool,

    #[arg(
        long,
        value_name = "TOKENS",
        help = "Fixed KV-cache capacity for decode (disables power-of-two growth buckets)"
    )]
    kv_cache_capacity: Option<usize>,

    #[arg(
        long,
        default_value_t = 0,
        value_name = "N",
        help = "Exclude the first N generated tokens from timing/profiling (warmup)"
    )]
    warmup_tokens: usize,
}

#[derive(ClapArgs, Clone)]
struct VisionInputArgs {
    #[arg(long)]
    input: Option<PathBuf>,

    #[arg(long, default_value = "input")]
    input_key: String,

    /// Generate a deterministic random input when `--input` is not provided.
    #[arg(long, default_value_t = 0)]
    seed: u64,

    #[arg(long, default_value_t = 1)]
    batch: usize,

    #[arg(long, default_value_t = 224)]
    image_size: usize,
}

#[derive(ClapArgs, Clone)]
struct ForwardArgs {
    /// Model checkpoint (self-describing `GPTRSCHK`).
    #[arg(long)]
    checkpoint: PathBuf,

    /// Tokenizer config to use with `--prompt`.
    #[arg(long)]
    tokenizer: Option<PathBuf>,

    /// Text prompt (requires `--tokenizer`).
    #[arg(long)]
    prompt: Option<String>,

    /// Comma-separated token ids, e.g. `--tokens 1,2,3`.
    #[arg(long, value_delimiter = ',')]
    tokens: Vec<usize>,

    #[command(flatten)]
    vision: VisionInputArgs,

    /// Optional output tensor archive path for logits ("logits" tensor).
    #[arg(long)]
    out: Option<PathBuf>,
}

#[derive(ClapArgs, Clone)]
struct ConvertArgs {
    /// Conversion target name (e.g., "c").
    #[arg(long)]
    target: String,

    /// Input PTIR program (.json or .ptir).
    #[arg(long)]
    ptir: PathBuf,

    /// Output directory for converted IR.
    #[arg(long)]
    out: PathBuf,

    /// Optional entrypoint override.
    #[arg(long)]
    entrypoint: Option<String>,
}

#[derive(ClapArgs, Clone)]
struct PatternsArgs {
    /// Optional pattern target filter (exact match).
    #[arg(long)]
    target: Option<String>,
}

fn load_tokenizer(path: impl AsRef<Path>) -> Result<Tokenizer> {
    let path = path.as_ref();
    let data = fs::read_to_string(path)
        .with_context(|| format!("failed to read tokenizer from {}", path.display()))?;
    let cfg = TokenizerConfig::from_json_str(&data).with_context(|| {
        format!(
            "invalid tokenizer config in {} (expected flat gpt-rs schema or Hugging Face tokenizer.json)",
            path.display()
        )
    })?;
    Ok(Tokenizer::from_config(cfg))
}

fn load_vision_input_tensor(args: &VisionInputArgs) -> Result<Tensor> {
    if let Some(path) = args.input.as_ref() {
        let tensors = TensorArchive::load(path)?;
        let input = tensors.get(&args.input_key).ok_or_else(|| {
            anyhow!(
                "input archive {} missing tensor '{}'",
                path.display(),
                args.input_key
            )
        })?;
        return Ok(input.clone());
    }

    ensure!(
        args.batch > 0,
        "--batch must be > 0 when generating a random input",
    );
    ensure!(
        args.image_size > 0,
        "--image-size must be > 0 when generating a random input",
    );

    let mut rng = ChaCha8Rng::seed_from_u64(args.seed);
    Ok(Tensor::randn(
        Shape::new([args.batch, 3, args.image_size, args.image_size]),
        1.0,
        &mut rng,
    ))
}

fn main() -> Result<()> {
    let args = Args::parse();
    let _trace_guard = args
        .dump_dir
        .as_ref()
        .map(|dir| install_dump_sink(dir, args.dump_mode))
        .transpose()?;

    #[cfg(feature = "conversion-c")]
    gpt_rs_backend_c::register_conversion_targets();

    if args.profile {
        profiling::reset();
    }

    if matches!(args.command, Command::Convert(_)) {
        return run_conversion(match args.command {
            Command::Convert(convert) => convert,
            _ => unreachable!(),
        });
    }

    if matches!(args.command, Command::Patterns(_)) {
        return run_patterns(match args.command {
            Command::Patterns(patterns) => patterns,
            _ => unreachable!(),
        });
    }

    let backend_env = args
        .backend
        .unwrap_or_else(|| env::var("GPTRS_BACKEND").unwrap_or_else(|_| "faer".to_string()));
    let command = args.command;

    #[cfg(feature = "conversion-c")]
    let supported = ["faer", "c"];
    #[cfg(not(feature = "conversion-c"))]
    let supported = ["faer"];

    match backend_env.trim() {
        "faer" => {
            let backend = Arc::new(FaerCpuBackend::create());
            run_with_backend(backend, command, args.profile)?;
        }
        "c" => {
            #[cfg(feature = "conversion-c")]
            {
                let backend = Arc::new(CBackend::new());
                run_with_backend(backend, command, args.profile)?;
            }
            #[cfg(not(feature = "conversion-c"))]
            {
                bail!(
                    "GPTRS_BACKEND='c' requires building gpt-rs-cli with --features conversion-c"
                );
            }
        }
        other => {
            bail!(
                "unknown backend '{}'; available: {}",
                other,
                supported.join(", ")
            );
        }
    }

    if args.profile {
        if let Some(report) = profiling::take_formatted_tables() {
            eprintln!("{}", report);
        } else {
            eprintln!(
                "profiling enabled but no report available; rebuild gpt-rs with profiler support"
            );
        }
    }

    Ok(())
}

fn run_with_backend<B: PortableBackend + 'static>(
    backend: Arc<B>,
    command: Command,
    profile: bool,
) -> Result<()> {
    match command {
        Command::Generate(args) => run_generate(&backend, &args, profile),
        Command::Forward(args) => run_forward(&backend, &args, profile),
        Command::Convert(_) => unreachable!(),
        Command::Patterns(_) => unreachable!(),
    }
}

fn run_patterns(args: PatternsArgs) -> Result<()> {
    let mut defs: Vec<_> = gpt_rs::backend::pattern::all_pattern_defs()
        .iter()
        .collect();
    defs.sort_by_key(|def| def.target);

    let mut printed = 0usize;
    for def in defs {
        if args
            .target
            .as_deref()
            .is_some_and(|target| target != def.target)
        {
            continue;
        }
        println!("{} -> {}::{}", def.target, def.module_path, def.view_name);
        for field in def.fields {
            if field.optional {
                println!("  {}: Option<{}>", field.name, field.view);
            } else {
                println!("  {}: {}", field.name, field.view);
            }
        }
        printed += 1;
    }

    if printed == 0 {
        if let Some(target) = args.target {
            println!("no patterns found for target {target}");
        } else {
            println!(
                "no patterns registered (did you compile gpt-rs with any #[ptir_pattern] sites?)"
            );
        }
    }

    Ok(())
}

fn run_generate<B: PortableBackend + 'static>(
    backend: &Arc<B>,
    args: &GenerateArgs,
    profile: bool,
) -> Result<()> {
    let namespace = model_namespace_for_backend(backend.as_ref());
    let model =
        runtime::load_model_with_namespace(Arc::clone(backend), &args.checkpoint, namespace)
            .with_context(|| format!("failed to load checkpoint {}", args.checkpoint.display()))?;
    let lm = model.as_causal_lm().ok_or_else(|| {
        anyhow!(
            "model kind '{}' does not support causal generation",
            model.kind()
        )
    })?;
    let tokenizer = load_tokenizer(&args.tokenizer)?;

    let prompt_tokens = tokenizer.encode(&args.prompt);
    ensure!(
        !prompt_tokens.is_empty(),
        "prompt produced an empty token sequence"
    );

    let mut sampler = Sampler::new(args.temperature);
    if args.greedy {
        sampler.temperature = 0.0;
    }
    if let Some(k) = args.top_k {
        sampler.top_k = Some(k.max(1));
    }

    let mut stdout = io::stdout();
    let warmup = args.warmup_tokens.min(args.max_tokens);

    if warmup == 0 {
        if profile {
            profiling::reset();
        }
        let t0 = std::time::Instant::now();

        let mut generator = Generator::new_with_kv_cache_capacity(
            lm,
            &sampler,
            &prompt_tokens,
            args.kv_cache,
            args.kv_cache_capacity,
        )?;

        for step in 0..args.max_tokens {
            let token = if step + 1 == args.max_tokens {
                generator.step_final()?
            } else {
                generator.step()?
            };
            let piece = tokenizer.decode(&[token]);
            stdout.write_all(piece.as_bytes())?;
            stdout.flush()?;
        }

        let out = generator.into_tokens();
        let elapsed = t0.elapsed();

        stdout.write_all(b"\n")?;
        stdout.flush()?;

        let emitted = out
            .len()
            .saturating_sub(prompt_tokens.len())
            .min(args.max_tokens);
        eprintln!(
            "Generated {} tokens in {:.2?} ({:.2} tokens/sec)",
            emitted,
            elapsed,
            (emitted as f64) / elapsed.as_secs_f64().max(1e-9),
        );

        return Ok(());
    }

    let mut generator = Generator::new_with_kv_cache_capacity(
        lm,
        &sampler,
        &prompt_tokens,
        args.kv_cache,
        args.kv_cache_capacity,
    )?;

    let measured_steps = args.max_tokens.saturating_sub(warmup);
    let warmup_is_final = measured_steps == 0 && warmup > 0;

    for step in 0..warmup {
        let token = if warmup_is_final && step + 1 == warmup {
            generator.step_final()?
        } else {
            generator.step()?
        };
        let piece = tokenizer.decode(&[token]);
        stdout.write_all(piece.as_bytes())?;
        stdout.flush()?;
    }

    if profile {
        profiling::reset();
    }
    let t0 = std::time::Instant::now();

    for step in 0..measured_steps {
        let token = if step + 1 == measured_steps {
            generator.step_final()?
        } else {
            generator.step()?
        };
        let piece = tokenizer.decode(&[token]);
        stdout.write_all(piece.as_bytes())?;
        stdout.flush()?;
    }

    let out = generator.into_tokens();
    let elapsed = t0.elapsed();

    stdout.write_all(b"\n")?;
    stdout.flush()?;

    let emitted = out
        .len()
        .saturating_sub(prompt_tokens.len())
        .min(args.max_tokens);
    let measured = emitted.saturating_sub(warmup);
    eprintln!(
        "Generated {} tokens ({} warmup) in {:.2?} ({:.2} tokens/sec)",
        measured,
        warmup,
        elapsed,
        (measured as f64) / elapsed.as_secs_f64().max(1e-9),
    );

    Ok(())
}

fn run_conversion(args: ConvertArgs) -> Result<()> {
    let program = load_ptir_program(&args.ptir)?;
    let target = conversion::get_conversion_target(&args.target).ok_or_else(|| {
        anyhow!(
            "unknown conversion target '{}'; available: {}",
            args.target,
            conversion::list_conversion_targets().join(", ")
        )
    })?;

    let options = ConversionOptions {
        entrypoint_override: args.entrypoint.clone(),
    };

    target
        .check(&program, &options)
        .map_err(|err| anyhow!(err.to_string()))?;
    let converted = target
        .convert(&program, &options)
        .map_err(|err| anyhow!(err.to_string()))?;

    fs::create_dir_all(&args.out)
        .with_context(|| format!("failed to create output dir {}", args.out.display()))?;
    let filename = format!("program.{}", target.file_extension());
    let out_path = args.out.join(filename);
    fs::write(&out_path, converted.module)
        .with_context(|| format!("failed to write {}", out_path.display()))?;

    let meta_path = args.out.join("meta.json");
    let meta = serde_json::json!({
        "target": target.name(),
        "entrypoints": converted.entrypoints.iter().map(|e| {
            serde_json::json!({ "ptir": e.ptir, "symbol": e.symbol })
        }).collect::<Vec<_>>(),
    });
    fs::write(&meta_path, serde_json::to_string_pretty(&meta)?)
        .with_context(|| format!("failed to write {}", meta_path.display()))?;

    eprintln!(
        "Converted PTIR '{}' to {}",
        args.ptir.display(),
        out_path.display()
    );
    Ok(())
}

fn load_ptir_program(path: &Path) -> Result<Program> {
    let ext = path.extension().and_then(|s| s.to_str()).unwrap_or("");
    match ext {
        "json" => Program::load_json(path).map_err(|err| anyhow!(err.to_string())),
        "ptir" | "txt" => {
            let src = fs::read_to_string(path)
                .with_context(|| format!("failed to read {}", path.display()))?;
            if src.trim_start().starts_with("program @") {
                bail!("text PTIR program parsing is not supported yet; use the JSON dump instead");
            }
            parse_program(&src).map_err(|err| anyhow!(err.to_string()))
        }
        _ => {
            let json = Program::load_json(path);
            if let Ok(program) = json {
                return Ok(program);
            }
            let src = fs::read_to_string(path)
                .with_context(|| format!("failed to read {}", path.display()))?;
            if src.trim_start().starts_with("program @") {
                bail!("text PTIR program parsing is not supported yet; use the JSON dump instead");
            }
            parse_program(&src).map_err(|err| anyhow!(err.to_string()))
        }
    }
}

fn run_forward<B: PortableBackend + 'static>(
    backend: &Arc<B>,
    args: &ForwardArgs,
    profile: bool,
) -> Result<()> {
    let namespace = model_namespace_for_backend(backend.as_ref());
    let mut model =
        runtime::load_model_with_namespace(Arc::clone(backend), &args.checkpoint, namespace)
            .with_context(|| format!("failed to load checkpoint {}", args.checkpoint.display()))?;

    let is_text = args.prompt.is_some() || !args.tokens.is_empty();
    if args.prompt.is_some() && !args.tokens.is_empty() {
        bail!("use either --prompt or --tokens, not both");
    }
    if args.tokenizer.is_some() && args.prompt.is_none() {
        bail!("--tokenizer is only valid together with --prompt");
    }
    if is_text && args.vision.input.is_some() {
        bail!("cannot combine text input (--prompt/--tokens) with vision input (--input)");
    }

    let input = if let Some(prompt) = args.prompt.as_ref() {
        let tokenizer_path = args
            .tokenizer
            .as_ref()
            .ok_or_else(|| anyhow!("--prompt requires --tokenizer"))?;
        let tokenizer = load_tokenizer(tokenizer_path)?;
        let tokens = tokenizer.encode(prompt);
        ensure!(
            !tokens.is_empty(),
            "prompt produced an empty token sequence"
        );
        runtime::ModelInput::Tokens(tokens)
    } else if !args.tokens.is_empty() {
        runtime::ModelInput::Tokens(args.tokens.clone())
    } else {
        let input = load_vision_input_tensor(&args.vision)?;
        let input_device = DeviceTensor::from_host(Arc::clone(backend), input)
            .with_context(|| "failed to move input to device")?;
        runtime::ModelInput::Vision(input_device)
    };

    if profile {
        profiling::reset();
    }
    let t0 = std::time::Instant::now();
    let runtime::ModelOutput::Tensor(host) = model.forward(input)?;
    let elapsed = t0.elapsed();

    let model_kind = model.kind();
    let shape = host.shape().dims();
    println!("model={} output_shape={:?}", model_kind, shape);

    if shape.len() == 2 && shape[0] > 0 && shape[1] > 0 {
        let rows = shape[0];
        let cols = shape[1];
        let row = if is_text { rows - 1 } else { 0 };
        let start = row * cols;
        let logits = &host.data()[start..start + cols];
        let mut best = 0usize;
        let mut best_val = f32::NEG_INFINITY;
        for (i, &v) in logits.iter().enumerate() {
            if v > best_val {
                best = i;
                best_val = v;
            }
        }
        println!("top1={} logit={:.6}", best, best_val);
    }

    eprintln!("wall_s={:.3}", elapsed.as_secs_f64());

    if let Some(out) = args.out.as_ref() {
        let mut map = HashMap::new();
        map.insert("logits".to_string(), host);
        TensorArchive::save(out, &map)
            .with_context(|| format!("failed to write output archive {}", out.display()))?;
    }

    Ok(())
}
