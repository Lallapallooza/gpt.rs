use anyhow::{anyhow, bail, ensure, Context, Result};
use clap::{Args as ClapArgs, Parser, Subcommand, ValueEnum};
use gpt_rs::backend::conversion::{self, ConversionOptions};
use gpt_rs::backend::spec::Program;
use gpt_rs::backend::text_ir::parse_program;
use gpt_rs::inference::generate::Generator;
use gpt_rs::inference::sampler::Sampler;
use gpt_rs::io::tensor_archive::TensorArchive;
use gpt_rs::ops::trace::{self, FileTraceOptions, FileTraceSink, ProgramDumpFilter};
use gpt_rs::profiling;
use gpt_rs::runtime;
use gpt_rs::tensor::{DeviceTensor, Shape, Tensor};
use gpt_rs::tokenizer::{Tokenizer, TokenizerConfig};
use gpt_rs::PortableBackend;
#[cfg(feature = "conversion-c")]
use gpt_rs_backend_c::CBackend;
use gpt_rs_backend_faer::FaerCpuBackend;
use rand::SeedableRng;
use rand_chacha::ChaCha8Rng;
use std::collections::HashMap;
use std::env;
use std::fs;
use std::io::{self, Write};
use std::path::{Path, PathBuf};
use std::sync::Arc;

fn install_dump_sink(path: &Path, dump_mode: DumpMode) -> Result<trace::TraceGuard> {
    let options = FileTraceOptions {
        dump_filter: match dump_mode {
            DumpMode::All => ProgramDumpFilter::AllExecutions,
            DumpMode::Compile => ProgramDumpFilter::CompileOnly,
        },
        ..Default::default()
    };

    let sink = FileTraceSink::with_options(path.to_path_buf(), options)
        .with_context(|| format!("failed to prepare PTIR dump directory {}", path.display()))?;
    Ok(trace::install_global_sink(Arc::new(sink)))
}

#[derive(Parser)]
#[command(version, about = "gpt.rs model runner")]
struct Args {
    #[command(subcommand)]
    command: Command,

    #[arg(
        long,
        value_name = "DIR",
        global = true,
        help = "Dump PTIR programs and metadata to the specified directory"
    )]
    dump_dir: Option<PathBuf>,

    #[arg(
        long,
        value_enum,
        global = true,
        default_value = "all",
        help = "Controls which PTIR executions are dumped (all, compile)"
    )]
    dump_mode: DumpMode,

    #[arg(long, global = true, help = "Print gpt-rs profiler tables to stderr")]
    profile: bool,
}

#[derive(Copy, Clone, Debug, ValueEnum)]
enum DumpMode {
    #[value(name = "all")]
    All,
    #[value(name = "compile")]
    Compile,
}

#[derive(Subcommand)]
enum Command {
    /// Run a model once (predict / generate).
    Run {
        #[command(subcommand)]
        model: RunModel,
    },

    /// Capture intermediate tensors for models that support tracing.
    Trace {
        #[command(subcommand)]
        model: TraceModel,
    },

    /// Convert a PTIR program into a target IR module.
    Convert(ConvertArgs),

    /// List PTIR patterns generated by `#[ptir_pattern]`.
    Patterns(PatternsArgs),
}

#[derive(Subcommand)]
enum RunModel {
    /// GPT-style text generation from a gpt.rs checkpoint + tokenizer.
    #[command(name = "gpt")]
    Gpt(RunGptArgs),
    /// ResNet34 forward pass from a gpt.rs checkpoint produced by the Python tools.
    #[command(name = "resnet34")]
    ResNet34(RunVisionArgs),
    /// MobileNetV2 forward pass from a gpt.rs checkpoint produced by the Python tools.
    #[command(name = "mobilenet_v2", alias = "mobilenet-v2")]
    MobileNetV2(RunVisionArgs),
}

#[derive(Subcommand)]
enum TraceModel {
    #[command(name = "resnet34")]
    ResNet34(TraceVisionArgs),
    #[command(name = "mobilenet_v2", alias = "mobilenet-v2")]
    MobileNetV2(TraceVisionArgs),
}

#[derive(ClapArgs, Clone)]
struct RunGptArgs {
    #[arg(long, default_value = "checkpoints/gpt2.bin")]
    checkpoint: PathBuf,

    #[arg(long, default_value = "configs/gpt2_tokenizer.json")]
    tokenizer: PathBuf,

    #[arg(long, default_value = "Hello")]
    prompt: String,

    #[arg(long, default_value_t = 128)]
    max_tokens: usize,

    #[arg(long, default_value_t = 0.8)]
    temperature: f32,

    #[arg(long, default_value_t = false)]
    greedy: bool,

    #[arg(long)]
    top_k: Option<usize>,

    #[arg(long, default_value_t = true)]
    kv_cache: bool,

    #[arg(
        long,
        value_name = "TOKENS",
        help = "Fixed KV-cache capacity for decode (disables power-of-two growth buckets)"
    )]
    kv_cache_capacity: Option<usize>,

    #[arg(
        long,
        default_value_t = 0,
        value_name = "N",
        help = "Exclude the first N generated tokens from timing/profiling (warmup)"
    )]
    warmup_tokens: usize,
}

#[derive(ClapArgs, Clone)]
struct RunVisionArgs {
    /// Model checkpoint produced by the export tools.
    #[arg(long)]
    checkpoint: PathBuf,

    /// Tensor archive containing a single input tensor (default key: "input").
    #[arg(long)]
    input: Option<PathBuf>,

    #[arg(long, default_value = "input")]
    input_key: String,

    /// Generate a deterministic random input when `--input` is not provided.
    #[arg(long, default_value_t = 0)]
    seed: u64,

    #[arg(long, default_value_t = 1)]
    batch: usize,

    #[arg(long, default_value_t = 224)]
    image_size: usize,

    /// Optional output tensor archive path for logits ("logits" tensor).
    #[arg(long)]
    out: Option<PathBuf>,
}

#[derive(ClapArgs, Clone)]
struct TraceVisionArgs {
    /// Model checkpoint produced by the export tools.
    #[arg(long)]
    checkpoint: PathBuf,

    /// Tensor archive containing a single input tensor (default key: "input").
    #[arg(long)]
    input: Option<PathBuf>,

    #[arg(long, default_value = "input")]
    input_key: String,

    /// Generate a deterministic random input when `--input` is not provided.
    #[arg(long, default_value_t = 0)]
    seed: u64,

    #[arg(long, default_value_t = 1)]
    batch: usize,

    #[arg(long, default_value_t = 224)]
    image_size: usize,

    /// Output tensor archive path (all traced tensors).
    #[arg(long)]
    out: PathBuf,
}

#[derive(ClapArgs, Clone)]
struct ConvertArgs {
    /// Conversion target name (e.g., "c").
    #[arg(long)]
    target: String,

    /// Input PTIR program (.json or .ptir).
    #[arg(long)]
    ptir: PathBuf,

    /// Output directory for converted IR.
    #[arg(long)]
    out: PathBuf,

    /// Optional entrypoint override.
    #[arg(long)]
    entrypoint: Option<String>,
}

#[derive(ClapArgs, Clone)]
struct PatternsArgs {
    /// Optional pattern target filter (exact match).
    #[arg(long)]
    target: Option<String>,
}

fn load_tokenizer(path: impl AsRef<Path>) -> Result<Tokenizer> {
    let path = path.as_ref();
    let data = fs::read_to_string(path)
        .with_context(|| format!("failed to read tokenizer from {}", path.display()))?;
    let cfg: TokenizerConfig = serde_json::from_str(&data)
        .with_context(|| format!("invalid tokenizer config in {}", path.display()))?;
    Ok(Tokenizer::from_config(cfg))
}

fn load_input_tensor(args: &RunVisionArgs) -> Result<Tensor> {
    if let Some(path) = args.input.as_ref() {
        let tensors = TensorArchive::load(path)?;
        let input = tensors.get(&args.input_key).ok_or_else(|| {
            anyhow!(
                "input archive {} missing tensor '{}'",
                path.display(),
                args.input_key
            )
        })?;
        return Ok(input.clone());
    }

    ensure!(
        args.batch > 0,
        "--batch must be > 0 when generating a random input",
    );
    ensure!(
        args.image_size > 0,
        "--image-size must be > 0 when generating a random input",
    );

    let mut rng = ChaCha8Rng::seed_from_u64(args.seed);
    Ok(Tensor::randn(
        Shape::new([args.batch, 3, args.image_size, args.image_size]),
        1.0,
        &mut rng,
    ))
}

fn load_input_tensor_trace(args: &TraceVisionArgs) -> Result<Tensor> {
    let run_args = RunVisionArgs {
        checkpoint: args.checkpoint.clone(),
        input: args.input.clone(),
        input_key: args.input_key.clone(),
        seed: args.seed,
        batch: args.batch,
        image_size: args.image_size,
        out: None,
    };
    load_input_tensor(&run_args)
}

fn main() -> Result<()> {
    let args = Args::parse();
    let _trace_guard = args
        .dump_dir
        .as_ref()
        .map(|dir| install_dump_sink(dir, args.dump_mode))
        .transpose()?;

    #[cfg(feature = "conversion-c")]
    gpt_rs_backend_c::register_conversion_targets();

    if args.profile {
        profiling::reset();
    }

    if matches!(args.command, Command::Convert(_)) {
        return run_conversion(match args.command {
            Command::Convert(convert) => convert,
            _ => unreachable!(),
        });
    }

    if matches!(args.command, Command::Patterns(_)) {
        return run_patterns(match args.command {
            Command::Patterns(patterns) => patterns,
            _ => unreachable!(),
        });
    }

    let backend_env = env::var("GPTRS_BACKEND").unwrap_or_else(|_| "faer".to_string());
    let command = args.command;

    #[cfg(feature = "conversion-c")]
    let supported = ["faer", "c"];
    #[cfg(not(feature = "conversion-c"))]
    let supported = ["faer"];

    match backend_env.trim() {
        "faer" => {
            let backend = Arc::new(FaerCpuBackend::create());
            run_with_backend(backend, command, args.profile)?;
        }
        "c" => {
            #[cfg(feature = "conversion-c")]
            {
                let backend = Arc::new(CBackend::new());
                run_with_backend(backend, command, args.profile)?;
            }
            #[cfg(not(feature = "conversion-c"))]
            {
                bail!(
                    "GPTRS_BACKEND='c' requires building gpt-rs-cli with --features conversion-c"
                );
            }
        }
        other => {
            bail!(
                "unknown backend '{}'; available: {}",
                other,
                supported.join(", ")
            );
        }
    }

    if args.profile {
        if let Some(report) = profiling::take_formatted_tables() {
            eprintln!("{}", report);
        } else {
            eprintln!(
                "profiling enabled but no report available; rebuild gpt-rs with profiler support"
            );
        }
    }

    Ok(())
}

fn run_with_backend<B: PortableBackend + 'static>(
    backend: Arc<B>,
    command: Command,
    profile: bool,
) -> Result<()> {
    match command {
        Command::Run { model } => match model {
            RunModel::Gpt(run) => run_gpt(&backend, &run, profile),
            RunModel::ResNet34(run) => run_vision_resnet34(&backend, &run, profile),
            RunModel::MobileNetV2(run) => run_vision_mobilenet_v2(&backend, &run, profile),
        },
        Command::Trace { model } => match model {
            TraceModel::ResNet34(run) => trace_vision_resnet34(&backend, &run, profile),
            TraceModel::MobileNetV2(run) => trace_vision_mobilenet_v2(&backend, &run, profile),
        },
        Command::Convert(_) => unreachable!(),
        Command::Patterns(_) => unreachable!(),
    }
}

fn run_patterns(args: PatternsArgs) -> Result<()> {
    let mut defs: Vec<_> = gpt_rs::backend::pattern::all_pattern_defs()
        .iter()
        .collect();
    defs.sort_by_key(|def| def.target);

    let mut printed = 0usize;
    for def in defs {
        if args
            .target
            .as_deref()
            .is_some_and(|target| target != def.target)
        {
            continue;
        }
        println!("{} -> {}::{}", def.target, def.module_path, def.view_name);
        for field in def.fields {
            if field.optional {
                println!("  {}: Option<{}>", field.name, field.view);
            } else {
                println!("  {}: {}", field.name, field.view);
            }
        }
        printed += 1;
    }

    if printed == 0 {
        if let Some(target) = args.target {
            println!("no patterns found for target {target}");
        } else {
            println!(
                "no patterns registered (did you compile gpt-rs with any #[ptir_pattern] sites?)"
            );
        }
    }

    Ok(())
}

fn run_gpt<B: PortableBackend + 'static>(
    backend: &Arc<B>,
    args: &RunGptArgs,
    profile: bool,
) -> Result<()> {
    let model = runtime::load_model_with_namespace(
        Arc::clone(backend),
        &args.checkpoint,
        runtime::next_namespace(),
    )
    .with_context(|| format!("failed to load checkpoint {}", args.checkpoint.display()))?;
    ensure!(
        model.kind() == "gpt",
        "expected model kind 'gpt', got '{}'",
        model.kind()
    );
    let lm = model.as_causal_lm().ok_or_else(|| {
        anyhow!(
            "model kind '{}' does not support causal generation",
            model.kind()
        )
    })?;
    let tokenizer = load_tokenizer(&args.tokenizer)?;

    let prompt_tokens = tokenizer.encode(&args.prompt);
    ensure!(
        !prompt_tokens.is_empty(),
        "prompt produced an empty token sequence"
    );

    let mut sampler = Sampler::new(args.temperature);
    if args.greedy {
        sampler.temperature = 0.0;
    }
    if let Some(k) = args.top_k {
        sampler.top_k = Some(k.max(1));
    }

    let mut stdout = io::stdout();
    let warmup = args.warmup_tokens.min(args.max_tokens);

    if warmup == 0 {
        if profile {
            profiling::reset();
        }
        let t0 = std::time::Instant::now();

        let mut generator = Generator::new_with_kv_cache_capacity(
            lm,
            &sampler,
            &prompt_tokens,
            args.kv_cache,
            args.kv_cache_capacity,
        )?;

        for step in 0..args.max_tokens {
            let token = if step + 1 == args.max_tokens {
                generator.step_final()?
            } else {
                generator.step()?
            };
            let piece = tokenizer.decode(&[token]);
            stdout.write_all(piece.as_bytes())?;
            stdout.flush()?;
        }

        let out = generator.into_tokens();
        let elapsed = t0.elapsed();

        stdout.write_all(b"\n")?;
        stdout.flush()?;

        let emitted = out
            .len()
            .saturating_sub(prompt_tokens.len())
            .min(args.max_tokens);
        eprintln!(
            "Generated {} tokens in {:.2?} ({:.2} tokens/sec)",
            emitted,
            elapsed,
            (emitted as f64) / elapsed.as_secs_f64().max(1e-9),
        );

        return Ok(());
    }

    let mut generator = Generator::new_with_kv_cache_capacity(
        lm,
        &sampler,
        &prompt_tokens,
        args.kv_cache,
        args.kv_cache_capacity,
    )?;

    let measured_steps = args.max_tokens.saturating_sub(warmup);
    let warmup_is_final = measured_steps == 0 && warmup > 0;

    for step in 0..warmup {
        let token = if warmup_is_final && step + 1 == warmup {
            generator.step_final()?
        } else {
            generator.step()?
        };
        let piece = tokenizer.decode(&[token]);
        stdout.write_all(piece.as_bytes())?;
        stdout.flush()?;
    }

    if profile {
        profiling::reset();
    }
    let t0 = std::time::Instant::now();

    for step in 0..measured_steps {
        let token = if step + 1 == measured_steps {
            generator.step_final()?
        } else {
            generator.step()?
        };
        let piece = tokenizer.decode(&[token]);
        stdout.write_all(piece.as_bytes())?;
        stdout.flush()?;
    }

    let out = generator.into_tokens();
    let elapsed = t0.elapsed();

    stdout.write_all(b"\n")?;
    stdout.flush()?;

    let emitted = out
        .len()
        .saturating_sub(prompt_tokens.len())
        .min(args.max_tokens);
    let measured = emitted.saturating_sub(warmup);
    eprintln!(
        "Generated {} tokens ({} warmup) in {:.2?} ({:.2} tokens/sec)",
        measured,
        warmup,
        elapsed,
        (measured as f64) / elapsed.as_secs_f64().max(1e-9),
    );

    Ok(())
}

fn run_conversion(args: ConvertArgs) -> Result<()> {
    let program = load_ptir_program(&args.ptir)?;
    let target = conversion::get_conversion_target(&args.target).ok_or_else(|| {
        anyhow!(
            "unknown conversion target '{}'; available: {}",
            args.target,
            conversion::list_conversion_targets().join(", ")
        )
    })?;

    let options = ConversionOptions {
        entrypoint_override: args.entrypoint.clone(),
    };

    target
        .check(&program, &options)
        .map_err(|err| anyhow!(err.to_string()))?;
    let converted = target
        .convert(&program, &options)
        .map_err(|err| anyhow!(err.to_string()))?;

    fs::create_dir_all(&args.out)
        .with_context(|| format!("failed to create output dir {}", args.out.display()))?;
    let filename = format!("program.{}", target.file_extension());
    let out_path = args.out.join(filename);
    fs::write(&out_path, converted.module)
        .with_context(|| format!("failed to write {}", out_path.display()))?;

    let meta_path = args.out.join("meta.json");
    let meta = serde_json::json!({
        "target": target.name(),
        "entrypoints": converted.entrypoints.iter().map(|e| {
            serde_json::json!({ "ptir": e.ptir, "symbol": e.symbol })
        }).collect::<Vec<_>>(),
    });
    fs::write(&meta_path, serde_json::to_string_pretty(&meta)?)
        .with_context(|| format!("failed to write {}", meta_path.display()))?;

    eprintln!(
        "Converted PTIR '{}' to {}",
        args.ptir.display(),
        out_path.display()
    );
    Ok(())
}

fn load_ptir_program(path: &Path) -> Result<Program> {
    let ext = path.extension().and_then(|s| s.to_str()).unwrap_or("");
    match ext {
        "json" => Program::load_json(path).map_err(|err| anyhow!(err.to_string())),
        "ptir" | "txt" => {
            let src = fs::read_to_string(path)
                .with_context(|| format!("failed to read {}", path.display()))?;
            if src.trim_start().starts_with("program @") {
                bail!("text PTIR program parsing is not supported yet; use the JSON dump instead");
            }
            parse_program(&src).map_err(|err| anyhow!(err.to_string()))
        }
        _ => {
            let json = Program::load_json(path);
            if let Ok(program) = json {
                return Ok(program);
            }
            let src = fs::read_to_string(path)
                .with_context(|| format!("failed to read {}", path.display()))?;
            if src.trim_start().starts_with("program @") {
                bail!("text PTIR program parsing is not supported yet; use the JSON dump instead");
            }
            parse_program(&src).map_err(|err| anyhow!(err.to_string()))
        }
    }
}

fn run_vision_resnet34<B: PortableBackend + 'static>(
    backend: &Arc<B>,
    args: &RunVisionArgs,
    profile: bool,
) -> Result<()> {
    run_vision_checkpoint("resnet34", backend, args, profile)
}

fn run_vision_mobilenet_v2<B: PortableBackend + 'static>(
    backend: &Arc<B>,
    args: &RunVisionArgs,
    profile: bool,
) -> Result<()> {
    run_vision_checkpoint("mobilenet_v2", backend, args, profile)
}

fn run_vision_checkpoint<B: PortableBackend + 'static>(
    expected_kind: &str,
    backend: &Arc<B>,
    args: &RunVisionArgs,
    profile: bool,
) -> Result<()> {
    let mut model = runtime::load_model_with_namespace(
        Arc::clone(backend),
        &args.checkpoint,
        runtime::next_namespace(),
    )
    .with_context(|| format!("failed to load checkpoint {}", args.checkpoint.display()))?;
    ensure!(
        model.kind() == expected_kind,
        "expected model kind '{}', got '{}'",
        expected_kind,
        model.kind()
    );
    run_vision_loaded_model(expected_kind, backend, model.as_mut(), args, profile)
}

fn run_vision_loaded_model<B: PortableBackend + 'static>(
    name: &str,
    backend: &Arc<B>,
    model: &mut dyn runtime::LoadedModel<B>,
    args: &RunVisionArgs,
    profile: bool,
) -> Result<()> {
    let input = load_input_tensor(args)?;
    let input_device = DeviceTensor::from_host(Arc::clone(backend), input)
        .with_context(|| "failed to move input to device")?;

    if profile {
        profiling::reset();
    }
    let t0 = std::time::Instant::now();
    let runtime::ModelOutput::Tensor(host) =
        model.forward(runtime::ModelInput::Vision(input_device))?;
    let elapsed = t0.elapsed();

    let shape = host.shape().dims();
    println!("model={} output_shape={:?}", name, shape);

    if shape.len() == 2 && shape[0] > 0 {
        let classes = shape[1];
        if classes > 0 {
            let row0 = &host.data()[0..classes];
            let mut best = 0usize;
            let mut best_val = f32::NEG_INFINITY;
            for (i, &v) in row0.iter().enumerate() {
                if v > best_val {
                    best = i;
                    best_val = v;
                }
            }
            println!("top1={} logit={:.6}", best, best_val);
        }
    }

    eprintln!("wall_s={:.3}", elapsed.as_secs_f64());

    if let Some(out) = args.out.as_ref() {
        let mut map = HashMap::new();
        map.insert("logits".to_string(), host);
        TensorArchive::save(out, &map)
            .with_context(|| format!("failed to write output archive {}", out.display()))?;
    }

    Ok(())
}

fn trace_vision_resnet34<B: PortableBackend + 'static>(
    backend: &Arc<B>,
    args: &TraceVisionArgs,
    profile: bool,
) -> Result<()> {
    trace_vision_checkpoint("resnet34", backend, args, profile)
}

fn trace_vision_mobilenet_v2<B: PortableBackend + 'static>(
    backend: &Arc<B>,
    args: &TraceVisionArgs,
    profile: bool,
) -> Result<()> {
    trace_vision_checkpoint("mobilenet_v2", backend, args, profile)
}

fn trace_vision_checkpoint<B: PortableBackend + 'static>(
    expected_kind: &str,
    backend: &Arc<B>,
    args: &TraceVisionArgs,
    profile: bool,
) -> Result<()> {
    let model = runtime::load_model_with_namespace(
        Arc::clone(backend),
        &args.checkpoint,
        runtime::next_namespace(),
    )
    .with_context(|| format!("failed to load checkpoint {}", args.checkpoint.display()))?;
    ensure!(
        model.kind() == expected_kind,
        "expected model kind '{}', got '{}'",
        expected_kind,
        model.kind()
    );

    println!("model={}", expected_kind);
    let input = load_input_tensor_trace(args)?;
    let input_device = DeviceTensor::from_host(Arc::clone(backend), input)
        .with_context(|| "failed to move input to device")?;

    if profile {
        profiling::reset();
    }
    let traced = model
        .trace_vision(&input_device)?
        .ok_or_else(|| anyhow!("model kind '{}' does not support tracing", model.kind()))?;

    let mut names = Vec::with_capacity(traced.len());
    let mut tensors = Vec::with_capacity(traced.len());
    for (trace_name, tensor) in traced {
        names.push(trace_name);
        tensors.push(tensor);
    }

    let refs = tensors.iter().collect::<Vec<_>>();
    let handles = DeviceTensor::materialize_many(&refs)
        .with_context(|| "failed to materialize trace outputs")?;

    let mut out = HashMap::new();
    for ((trace_name, tensor), handle) in names
        .into_iter()
        .zip(tensors.iter())
        .zip(handles.into_iter())
    {
        let literal = backend.to_literal(&handle)?;
        let host = Tensor::from_literal(&literal)?.requires_grad(tensor.requires_grad_flag());
        println!("trace {} shape={:?}", trace_name, host.shape().dims());
        out.insert(trace_name, host);
    }

    TensorArchive::save(&args.out, &out)
        .with_context(|| format!("failed to write trace archive {}", args.out.display()))?;
    Ok(())
}
