# Frontend Execution Model

The frontend of `gpt.rs` is structured as a small set of layers that cooperate to
produce portable forward passes (and keep the door open for future autograd):

- **Models** wire blocks together and define what is traced/returned.
- **Layers** encapsulate model parameters, book‑keeping, and high-level control flow.
  They expose ergonomic `forward` methods and are responsible for saving the
  minimal state required for potential future gradient support.
- **Functional implementations** provide side‑effect free math building blocks.  Each
  functional breaks a composite operation (for example multi‑head attention or layer
  normalisation) into elementary tensor primitives that are supplied by a backend.
- **Backends** own the actual tensor kernels.  They implement the primitive operations
  that the functionals request and can target different devices or optimised libraries.

This document explains how those pieces interact, how PTIR flows through the stack,
and where runtime functional overrides hook in.

## Models: wiring blocks

Model types live in `crates/gpt-rs/src/model/**` and are responsible for composing layers
into end-to-end networks.

Examples:

- GPT uses transformer blocks (attention + MLP + layer norms) and exposes autoregressive
  decoding helpers (kv-cache, `Generator`).
- ResNet34 and MobileNetV2 compose conv/bn/activation/pool blocks and expose
  `forward_trace` helpers to dump intermediate activations for Torch comparisons.

## Layers: high-level orchestration

Layer types live in `crates/gpt-rs/src/nn/layers/**`.  Each layer holds an
`Arc<B>` for some backend `B: PortableBackend` together with its learnable weights and exposes
methods such as:

- `forward(&self, input: &DeviceTensor<B>) -> Result<DeviceTensor<B>>`
- `forward_with_state(&self, input: &DeviceTensor<B>) -> Result<(DeviceTensor<B>, State)>`

The layer is responsible for:

1. **Shape and configuration validation** – e.g. `CausalSelfAttention` asserts that the
   embedding dimension is divisible by the head count before doing any work.
2. **State capture for potential reverse mode** – only the minimal tensors required for future
   gradient propagation are stored (`LinearState` keeps the original input, `CausalSelfAttentionState`
   retains the cached projections and runtime cache entries for reuse, etc.).
3. **Delegation to functionals** – heavy lifting (attention kernels, layer norm math,
   etc.) is routed through the functional registry so that layers remain orchestration
   only.  Layers never open-code GEMMs or reductions themselves; they always call the
   backend through a functional façade.

This separation keeps the public API expressive (layers look like standard deep-learning
modules) while still allowing lower layers to swap out implementations.

## Functional implementations: pure math + registry

Functionals live under `crates/gpt-rs/src/ops/functional/**` and are intentionally pure:

- Each is a plain free function annotated with `#[support_runtime_overload]`. The macro
  synthesises the context wrapper, registry entry, and default implementation wiring.
- They return concrete tensors and caches without mutating global state (for example
  `functional::attention` now yields both the present key/value chunk and the accumulated
  `AttentionCache`).
- They call only backend primitives such as `matmul`, `softmax_last_dim`, or PTIR
  programs.  This means the same functional works with any backend that honours the
  primitive contract.

The arithmetic family (`add`, `sub`, `mul`, `div`, `maximum`, `minimum`, plus the unary
`neg`/`abs` operations and `clamp` with optional bounds) validates shape and dtype equality up front and
then lowers to `ElementwiseBinary`/`ElementwiseUnary` PTIR programs.  The shared `matmul`
functional covers both rank-2 GEMM and rank-3 batched GEMM: it enforces matching contract
dimensions, shared backends/dtypes, and emits a `dot_general` program that yields `[M, N]` or
`[B, M, N]` results as appropriate.  Any backend that implements those primitives picks up
the behaviour automatically.

### Device tensor extension methods

`DeviceTensorOps` is implemented for `DeviceTensor<B>` and re-exported from `gpt_rs::tensor`.
Import the trait to unlock zero-cost method syntax such as
`tensor.add(&other)?.clamp(None, Some(&limit))`; calls still route through the functional
definitions, so layers remain backend-agnostic.

A `FunctionalRegistry` keeps the available implementations per operation family. When a
layer asks for attention, the registry selects the first implementation whose
`supports(...)` predicate returns true. The code generated by
`#[support_runtime_overload]` automatically registers the portable reference on demand,
so layers simply call `functional::attention(backend, &config, &qkv, cache)` with an optional
`AttentionCache`.

### Overriding functionals

Users can inject custom kernels without editing layer code:

```rust
let registry = FunctionalRegistry::default();
registry.register_attention(
    AttentionImplementation::<Backend>::new("cuda_attention", cuda_attention_forward)
        .with_supports(cuda_attention_supports),
);
```

Registries are installed via the thread-local runtime stack:

```rust
let _guard = gpt_rs::ops::functional::runtime::push_registry(std::sync::Arc::new(registry));
// run model forward here
```

Alternatively, `FunctionalOverrides` can be loaded from configuration to select a named
implementation at runtime.  This is how GPU integrations can reuse fused kernels (for
instance, a single-call layer norm) while still falling back to the reference
implementation for unsupported shapes.

As long as the custom functional restricts itself to backend primitives it inherits all
of the automatic shape validation and state capture that the layer already performs.

## Backends: primitive providers

Backends execute the primitive PTIR op set that functionals lower into (e.g. `dot_general`,
`reduce_window`, `extract_patches`, elementwise ops, gathers, reshapes).

Only the PTIR-based contract (`gpt_rs::backend::spec::PortableBackend`) lives in the core crate;
implementations live in sibling crates:

- `gpt-rs-backend-faer`: optimized CPU backend (recommended).
- `gpt-rs-backend-ref-cpu`: slow reference interpreter for debugging/spec bring-up.

Because layers interact only with an `Arc<B>` for `B: PortableBackend`, swapping implementations is
purely a matter of providing a different backend instance. Backends can also be wrapped with hooks
for profiling/debugging (see `docs/testing.md`).

## Putting it together

A typical forward pass flows like this:

1. Application code calls `CausalSelfAttention::forward(x)` (or
   `CausalSelfAttention::forward_with_cache(x, cache)` during autoregressive decoding).
2. The layer projects Q/K/V using backend primitives, then hands the result to
   `functional::attention(backend, &config, &qkv, cache)`.
3. The functional registry selects an implementation (default: reference) and runs it,
   invoking multiple backend primitives (or a single fused kernel) to compute the
   context tensor and cache (including an updated `AttentionCache` ready for reuse on
   the next autoregressive step).
4. The layer finishes its bookkeeping and returns the output alongside any state that downstream
   consumers might reuse.

This decomposition keeps the codebase welcoming to experimentation: layers read like the
math in the paper, functionals isolate numerical recipes, and backends provide the knobs
needed to target new hardware or fuse operations – all while preserving a correctness
baseline via the reference CPU path.
